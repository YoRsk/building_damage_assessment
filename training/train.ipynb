{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-29T10:51:39.790345Z",
     "start_time": "2024-08-29T10:51:39.781759Z"
    }
   },
   "source": [
    "from model.my_models import SiameseUNetWithResnet50Encoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from loss_functions import FocalLoss"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T10:51:26.990371Z",
     "start_time": "2024-08-29T10:51:16.404311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# in tensorboard see the architecture of model \n",
    "# vit对输入图像大小有严格要求，需要是32的倍数，所以这里使用256*256的图像\n",
    "from torchvision.models import resnet50\n",
    "#tensorboard\n",
    "#命令行输入 tensorboard --logdir=./training/runs\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('./runs')\n",
    "model = SiameseUNetWithResnet50Encoder()\n",
    "\n",
    "# 定义输入张量，假设输入为两张 256x256 的 RGB 图像\n",
    "input_1 = torch.rand(1, 3, 256, 256)\n",
    "input_2 = torch.rand(1, 3, 256, 256)\n",
    "# 不加torch.no_grad()会报错\n",
    "with torch.no_grad():\n",
    "    writer.add_graph(model, (input_1, input_2))\n",
    "# with torch.no_grad():\n",
    "#     writer.add_graph(model, input_to_model = torch.rand(1, 3, 256, 256))\n",
    "writer.close()"
   ],
   "id": "a7086be02cebd151",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "post_dir_img = Path('.\\\\Dataset\\\\TierFull\\\\Post\\\\Image512\\\\')\n",
    "post_dir_mask = Path('.\\\\Dataset\\\\TierFull\\\\Post\\\\Label512\\\\')\n",
    "pre_dir_img = Path('.\\\\Dataset\\\\TierFull\\\\Pre\\\\Image512\\\\')\n",
    "pre_dir_mask = Path('.\\\\Dataset\\\\TierFull\\\\Pre\\\\Label512\\\\')\n",
    "post_dir_val_img = Path('.\\\\Dataset\\\\Validation\\\\Post\\\\Image512\\\\')\n",
    "post_dir_val_mask = Path('.\\\\Dataset\\\\Validation\\\\Post\\\\Label512\\\\')\n",
    "pre_dir_val_img = Path('.\\\\Dataset\\\\Validation\\\\Pre\\\\Image512\\\\')\n",
    "pre_dir_val_mask = Path('.\\\\Dataset\\\\Validation\\\\Pre\\\\Label512\\\\')\n",
    "dir_checkpoint = Path('./checkpoints/Vgg19SiamConc/')"
   ],
   "id": "facb3659ae0b68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "closs = nn.CrossEntropyLoss()\n",
    "\n",
    "floss = FocalLoss(mode = 'multiclass',\n",
    "                alpha = None,\n",
    "                gamma = 2.0,\n",
    "                ignore_index = None,\n",
    "                reduction = \"mean\",\n",
    "                normalized = False,\n",
    "                reduced_threshold = None)"
   ],
   "id": "a1606a6cbf45f917"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5bd79b74dcc1a11"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
